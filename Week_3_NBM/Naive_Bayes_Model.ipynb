{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes text classification\n",
    "\n",
    "Naive Bayes model with TF/IDF algorithm to solve text classification problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency\n",
    "TF — это частотность термина, которая измеряет, насколько часто термин встречается в документе. Логично предположить, что в длинных документах термин может встретиться в больших количествах, чем в коротких, поэтому абсолютные числа тут не катят. Поэтому применяют относительные — делят количество раз, когда нужный термин встретился в тексте, на общее количество слов в тексте. \n",
    "\n",
    "## Inverse Document Frequency\n",
    "IDF — это обратная частотность документов. Она измеряет непосредственно важность термина. То есть, когда мы считали TF, все термины считаются как бы равными по важности друг другу. Но всем известно, что, например, предлоги встречаются очень часто, хотя практически не влияют на смысл текста. И что с этим поделать? Ответ прост — посчитать IDF. Он считается как логарифм от общего количества документов, делённого на количество документов, в которых встречается термин а.\n",
    "\n",
    "#### TF термина а = (Количество раз, когда термин а встретился в тексте / количество всех слов в тексте)\n",
    "#### IDF термина а = (Общее количество документов / Количество документов, в которых встречается термин а)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self, data):\n",
    "        self.corpus, self.labels  = self.data_split(data)\n",
    "        self.classes = Counter(self.labels)\n",
    "     \n",
    "    def data_split(self, data):\n",
    "        corpus = []\n",
    "        labels = []\n",
    "        for text, label in data:\n",
    "            corpus.append(text.split())\n",
    "            labels.append(label)\n",
    "        return corpus, labels\n",
    "    \n",
    "    def get_unique_words(self):\n",
    "        unique_words_in_corpus = Counter()\n",
    "        for doc in self.corpus:\n",
    "            unique_words_in_corpus += Counter(doc)\n",
    "        return len(unique_words_in_corpus)\n",
    "    \n",
    "    def get_class_words(self):\n",
    "        total = defaultdict(int)\n",
    "        words = defaultdict(Counter)       \n",
    "        for i in range(len(self.corpus)):           \n",
    "            words[self.labels[i]] += Counter(self.corpus[i])\n",
    "            total[self.labels[i]] += len(self.corpus[i])\n",
    "        return words, total\n",
    "    \n",
    "    def get_prior (self, class_label):\n",
    "        return self.classes[class_label]/len(self.corpus)          \n",
    "    \n",
    "    def get_p (self, class_label, word):\n",
    "        p = (self.words[class_label][word] + 1)/(self.total[class_label] + self.unique)\n",
    "        return p #math.log(p)\n",
    "    \n",
    "    def fit(self):\n",
    "        self.words, self.total = self.get_class_words()\n",
    "        self.unique = self.get_unique_words()\n",
    "            \n",
    "    def predict_doc(self, doc):\n",
    "        p = dict()\n",
    "        for label in self.classes:\n",
    "            p[label] = self.get_prior (label)\n",
    "            for word in doc.split():\n",
    "                p[label] *= self.get_p(label,word)\n",
    "        y = max(p, key=p.get)\n",
    "        return y, p \n",
    "    \n",
    "    def predict(self, docs):\n",
    "        matches = np.zeros(len(docs))\n",
    "        for doc, label in docs:\n",
    "            y,p = self.predict_doc(doc)\n",
    "            matches[i] = (y == label)\n",
    "        return matches\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'0': 0.00030121377997263036, '1': 0.00013548070246744226}\n"
     ]
    }
   ],
   "source": [
    "# Train data\n",
    "data = [[\"Chinese Beijing Chinese\",\"0\"],\n",
    "        [\"Chinese Chinese Shanghai\",\"0\"],\n",
    "        [\"Chinese Macao\",\"0\"],\n",
    "        [\"Tokyo Japan Chinese\",\"1\"]]\n",
    "pred =  \"Chinese Chinese Chinese Tokyo Japan\"\n",
    "\n",
    "nbm = NaiveBayes(data)\n",
    "nbm.fit()\n",
    "y,p = nbm.predict_doc(pred)\n",
    "print(y,p)\n",
    "\n",
    "# model\n",
    "#nbm = NaiveBayes(data_train)\n",
    "#nbm.fit()\n",
    "#matches = nbm.predict(data_test)\n",
    "#print (\"Accuracy: \", matches.mean()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must return[ ('Chinese Chinese Chinese Tokyo Japan', '0')]\n",
    "# pobability {'1': 0.00013548070246744226, '0': 0.00030121377997263036}\n",
    "# or log     {'1': -7.906681345001262, '0': -7.10769031284391}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'pasta': 0.5, 'la': 0.3333333333333333, 'vista': 1.0, 'baby': 0.16666666666666666}, {'hasta': 0.5, 'siempre': 0.5, 'comandante': 0.25, 'baby': 0.16666666666666666, 'la': 0.16666666666666666}, {'siempre': 0.6000000000000001, 'comandante': 0.30000000000000004, 'baby': 0.2, 'la': 0.2}]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "\n",
    "def calc_tf(docum):\n",
    "    \"\"\"\n",
    "    parameters: docum - List of words\n",
    "    returns: Counter object with TF for all words in docum\n",
    "    \"\"\"\n",
    "    tf = Counter(docum)\n",
    "    for i in tf:\n",
    "        tf[i] = tf[i]/float(len(docum))\n",
    "    return tf\n",
    "\n",
    "def calc_idf(word, corpus):\n",
    "    \"\"\"\n",
    "    parameters: corpus - List of texts\n",
    "                word - or which to calculate IDF\n",
    "    returns: value, idf for word in corpus            \n",
    "    \"\"\"\n",
    "    word_in_doc_count = sum([1.0 for i in corpus if word in i])\n",
    "    idf = word_in_doc_count/len(corpus)\n",
    "    return idf\n",
    "        \n",
    "def calc_tfidf(corpus):\n",
    "    docs_list = []\n",
    "    for doc in corpus:\n",
    "        tf_idf_dict = {}\n",
    "        tf = calc_tf(doc)\n",
    "        for word in tf:\n",
    "            tf_idf_dict[word] = tf[word] / calc_idf(word, corpus)\n",
    "        docs_list.append(tf_idf_dict)\n",
    "    return docs_list\n",
    "\n",
    "texts = [['pasta', 'la', 'vista', 'baby', 'la', 'vista'], \n",
    "         ['hasta', 'siempre', 'comandante', 'baby', 'la', 'siempre'], \n",
    "         ['siempre', 'comandante', 'baby', 'la', 'siempre']]\n",
    "print (calc_tfidf(texts))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://nlpx.net/archives/57\n",
    "- https://stevenloria.com/tf-idf/\n",
    "- https://ru.wikipedia.org/wiki/TF-IDF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
