{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes text classification\n",
    "\n",
    "Naive Bayes model with TF/IDF algorithm to solve text classification problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import math\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_data = [[\"Chinese Beijing Chinese\",\"0\"],\n",
    "             [\"Chinese Chinese Shanghai\",\"0\"],\n",
    "             [\"Chinese Macao\",\"0\"],\n",
    "             [\"Tokyo Japan Chinese\",\"1\"]]\n",
    "demo_pred =  \"Chinese Chinese Chinese Tokyo Japan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self, data, isUseLog = False):\n",
    "        self.corpus, self.labels  = self.data_split(data)\n",
    "        self.classes = Counter(self.labels)\n",
    "        self.isUseLog = isUseLog\n",
    "     \n",
    "    def data_split(self, data):\n",
    "        corpus = []\n",
    "        labels = []\n",
    "        for text, label in data:\n",
    "            corpus.append(text.lower().split())\n",
    "            labels.append(label)\n",
    "        return corpus, labels\n",
    "    \n",
    "    def get_data_statistic(self):\n",
    "        print (\"*** NaiveBayes data statistic ***\")\n",
    "        print (\"Corpus length = \", len(self.corpus))\n",
    "        print (\"classes count = \", dict(self.classes))\n",
    "        print (\"classes ratio = \", {i: self.classes[i]/len(self.corpus) for i in self.classes} )\n",
    "        print (\"unique words in corpus = \", self.get_unique_words())\n",
    "        print (\"*** ------------------------- ***\")\n",
    "    \n",
    "    def get_unique_words(self):\n",
    "        unique_words_in_corpus = Counter()\n",
    "        for doc in self.corpus:\n",
    "            unique_words_in_corpus += Counter(doc)\n",
    "        return len(unique_words_in_corpus)\n",
    "    \n",
    "    def get_class_words(self):\n",
    "        total = defaultdict(int)\n",
    "        words = defaultdict(Counter)       \n",
    "        for i in range(len(self.corpus)):           \n",
    "            words[self.labels[i]] += Counter(self.corpus[i])\n",
    "            total[self.labels[i]] += len(self.corpus[i])\n",
    "        return words, total\n",
    "    \n",
    "    def get_prior (self, class_label):\n",
    "        return self.classes[class_label]/len(self.corpus)          \n",
    "    \n",
    "    def get_p (self, class_label, word):\n",
    "        # P(word|class) = (word_count_in_class + 1)/(total_words_in_class+total_unique_words_in_corpus) \n",
    "        p = (self.words[class_label][word] + 1)/(self.total[class_label] + self.unique)\n",
    "        return p\n",
    "    \n",
    "    def fit(self):\n",
    "        self.words, self.total = self.get_class_words()\n",
    "        self.unique = self.get_unique_words()\n",
    "            \n",
    "    def predict_doc(self, doc):\n",
    "        p = dict()\n",
    "        for label in self.classes:\n",
    "            p[label] = self.get_prior (label)\n",
    "            if self.isUseLog:\n",
    "                p[label] = math.log(p[label])\n",
    "            for word in doc.lower().split():\n",
    "                if self.isUseLog:\n",
    "                    p[label] += math.log(self.get_p(label,word))\n",
    "                else:\n",
    "                    p[label] *= self.get_p(label,word)\n",
    "        y = max(p, key=p.get)\n",
    "        return y, p \n",
    "    \n",
    "    def predict(self, docs):\n",
    "        matches = []\n",
    "        for doc, label in docs:\n",
    "            y,p = self.predict_doc(doc)\n",
    "            matches.append(y == label)\n",
    "        return Counter(matches)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** NaiveBayes data statistic ***\n",
      "Corpus length =  4\n",
      "classes count =  {'0': 3, '1': 1}\n",
      "classes ratio =  {'0': 0.75, '1': 0.25}\n",
      "unique words in corpus =  6\n",
      "*** ------------------------- ***\n",
      "pobability    0 {'0': 0.00030121377997263036, '1': 0.00013548070246744226}\n",
      "log           0 {'0': -8.10769031284391, '1': -8.906681345001262}\n"
     ]
    }
   ],
   "source": [
    "# Test NaiveBayes Model for demo data\n",
    "\n",
    "# pobability\n",
    "nbm = NaiveBayes(demo_data)\n",
    "nbm.get_data_statistic()\n",
    "nbm.fit()\n",
    "y,p = nbm.predict_doc(demo_pred)\n",
    "print(\"pobability   \", y, p)\n",
    "\n",
    "# log\n",
    "nbm = NaiveBayes(demo_data, True)\n",
    "nbm.fit()\n",
    "y,p = nbm.predict_doc(demo_pred)\n",
    "print(\"log          \", y, p)\n",
    "\n",
    "# Must return[ ('Chinese Chinese Chinese Tokyo Japan', '0')]\n",
    "# pobability {'1': 0.00013548070246744226, '0': 0.00030121377997263036}\n",
    "# or log     {'1': -7.906681345001262, '0': -7.10769031284391}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118 = 894 + 224\n",
      "------------\n",
      "*** NaiveBayes data statistic ***\n",
      "Corpus length =  1118\n",
      "classes count =  {'0': 380, '1': 738}\n",
      "classes ratio =  {'0': 0.33989266547406083, '1': 0.6601073345259392}\n",
      "unique words in corpus =  33697\n",
      "*** ------------------------- ***\n",
      "*** NaiveBayes data statistic ***\n",
      "Corpus length =  894\n",
      "classes count =  {'0': 297, '1': 597}\n",
      "classes ratio =  {'0': 0.33221476510067116, '1': 0.6677852348993288}\n",
      "unique words in corpus =  26275\n",
      "*** ------------------------- ***\n",
      "*** NaiveBayes data statistic ***\n",
      "Corpus length =  224\n",
      "classes count =  {'1': 141, '0': 83}\n",
      "classes ratio =  {'1': 0.6294642857142857, '0': 0.3705357142857143}\n",
      "unique words in corpus =  15295\n",
      "*** ------------------------- ***\n",
      "Wall time: 1.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# data.csv prepare dataset\n",
    "\n",
    "def read_csv_file(file_name):\n",
    "    with open(file_name, 'r') as csv_file:\n",
    "        reader = csv.reader(csv_file)\n",
    "        data = [doc for doc in reader]\n",
    "        csv_file.close()    \n",
    "    return data\n",
    "\n",
    "def delete_stopwords(data):\n",
    "    # nltk.download('stopwords')  # 1 time or nltk.download()\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords) + r')\\b\\s*')\n",
    "    for i in range(len(data)):   \n",
    "        data[i][0] = data[i][0].lower()\n",
    "        data[i][0] = pattern.sub('', data[i][0])   # filter(lambda x: x not in stopwords, data[i][0])\n",
    "    return data\n",
    "\n",
    "def train_verif_split(data, train_persent):\n",
    "    train_count = int(len(data)*train_persent/100.0)\n",
    "    train = data[:train_count]\n",
    "    verif = data[train_count:]\n",
    "    return train, verif\n",
    "\n",
    "# print statistic\n",
    "data = read_csv_file(\"data.csv\")\n",
    "train, verif = train_verif_split (data, 80.0)\n",
    "\n",
    "print(len(data), \"=\", len(train), \"+\", len(verif))\n",
    "print(\"------------\")\n",
    "\n",
    "nbm = NaiveBayes(data)\n",
    "nbm.get_data_statistic()\n",
    "\n",
    "nbm = NaiveBayes(train)\n",
    "nbm.get_data_statistic()\n",
    "\n",
    "nbm = NaiveBayes(verif)\n",
    "nbm.get_data_statistic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pobability\n",
      "Matches:   Counter({True: 117, False: 107})\n",
      "Accuracy:  {False: 0.47767857142857145, True: 0.5223214285714286}\n",
      "----------\n",
      "log\n",
      "Matches:   Counter({True: 214, False: 10})\n",
      "Accuracy:  {True: 0.9553571428571429, False: 0.044642857142857144}\n",
      "----------\n",
      "log without stopwords\n",
      "Matches:   Counter({True: 211, False: 13})\n",
      "Accuracy:  {True: 0.9419642857142857, False: 0.05803571428571429}\n",
      "----------\n",
      "Wall time: 4.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# data.csv model\n",
    "data = read_csv_file(\"data.csv\")\n",
    "train, verif = train_verif_split (data, 80.0)\n",
    "\n",
    "nbm = NaiveBayes(train)\n",
    "nbm.fit()\n",
    "matches = nbm.predict(verif)\n",
    "print (\"pobability\")\n",
    "print (\"Matches:  \", matches) \n",
    "print (\"Accuracy: \", {i: matches[i]/len(verif) for i in matches}) \n",
    "print (\"----------\")\n",
    "\n",
    "nbm = NaiveBayes(train, True)\n",
    "nbm.fit()\n",
    "matches = nbm.predict(verif)\n",
    "print (\"log\")\n",
    "print (\"Matches:  \", matches) \n",
    "print (\"Accuracy: \", {i: matches[i]/len(verif) for i in matches}) \n",
    "print (\"----------\")\n",
    "\n",
    "nbm = NaiveBayes(delete_stopwords(train), True)\n",
    "nbm.fit()\n",
    "matches = nbm.predict(delete_stopwords(verif))\n",
    "print (\"log without stopwords\")\n",
    "print (\"Matches:  \", matches) \n",
    "print (\"Accuracy: \", {i: matches[i]/len(verif) for i in matches}) \n",
    "print (\"----------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF algorithm\n",
    "\n",
    "## Term Frequency\n",
    "TF — это частотность термина, которая измеряет, насколько часто термин встречается в документе. Логично предположить, что в длинных документах термин может встретиться в больших количествах, чем в коротких, поэтому абсолютные числа тут не катят. Поэтому применяют относительные — делят количество раз, когда нужный термин встретился в тексте, на общее количество слов в тексте. \n",
    "\n",
    "## Inverse Document Frequency\n",
    "IDF — это обратная частотность документов. Она измеряет непосредственно важность термина. То есть, когда мы считали TF, все термины считаются как бы равными по важности друг другу. Но всем известно, что, например, предлоги встречаются очень часто, хотя практически не влияют на смысл текста. И что с этим поделать? Ответ прост — посчитать IDF. Он считается как логарифм от общего количества документов, делённого на количество документов, в которых встречается термин а.\n",
    "\n",
    "#### TF термина а = (Количество раз, когда термин а встретился в тексте / количество всех слов в тексте)\n",
    "#### IDF термина а = (Общее количество документов / Количество документов, в которых встречается термин а)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['chinese', 'beijing', 'chinese'], ['chinese', 'chinese', 'shanghai'], ['chinese', 'macao'], ['tokyo', 'japan', 'chinese']]\n",
      "[{'pasta': 0.05555555555555555, 'la': 0.3333333333333333, 'vista': 0.1111111111111111, 'baby': 0.16666666666666666}, {'hasta': 0.05555555555555555, 'siempre': 0.2222222222222222, 'comandante': 0.1111111111111111, 'baby': 0.16666666666666666, 'la': 0.16666666666666666}, {'siempre': 0.26666666666666666, 'comandante': 0.13333333333333333, 'baby': 0.2, 'la': 0.2}]\n",
      "---------------\n",
      "[{'chinese': 0.6666666666666666, 'beijing': 0.08333333333333333}, {'chinese': 0.6666666666666666, 'shanghai': 0.08333333333333333}, {'chinese': 0.5, 'macao': 0.125}, {'tokyo': 0.08333333333333333, 'japan': 0.08333333333333333, 'chinese': 0.3333333333333333}]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "\n",
    "nbm = NaiveBayes(demo_data)\n",
    "print(nbm.corpus)\n",
    "\n",
    "def calc_tf(doc):\n",
    "    \"\"\"\n",
    "    parameters: doc - List of words\n",
    "    returns: Counter object with TF for all words in docum\n",
    "    \"\"\"\n",
    "    tf = Counter(doc)\n",
    "    for i in tf:\n",
    "        tf[i] = tf[i]/float(len(doc))\n",
    "    return tf\n",
    "\n",
    "def calc_idf(word, corpus):\n",
    "    \"\"\"\n",
    "    parameters: corpus - List of docs\n",
    "                word - or which to calculate IDF\n",
    "    returns: value, idf for word in corpus            \n",
    "    \"\"\"\n",
    "    word_in_doc_count = sum([1.0 for i in corpus if word in i])\n",
    "    idf = len(corpus) / word_in_doc_count\n",
    "    return idf # math.log(idf)\n",
    "      \n",
    "def calc_tfidf(corpus):\n",
    "    docs_list = []\n",
    "    for doc in corpus:\n",
    "        tf_idf_dict = {}\n",
    "        tf = calc_tf(doc)\n",
    "        for word in tf:\n",
    "            tf_idf_dict[word] = tf[word] / calc_idf(word, corpus)\n",
    "        docs_list.append(tf_idf_dict)\n",
    "    return docs_list\n",
    "\n",
    "texts = [['pasta', 'la', 'vista', 'baby', 'la', 'vista'], \n",
    "         ['hasta', 'siempre', 'comandante', 'baby', 'la', 'siempre'], \n",
    "         ['siempre', 'comandante', 'baby', 'la', 'siempre']]\n",
    "print (calc_tfidf(texts))\n",
    "print (\"---------------\")\n",
    "print (calc_tfidf(nbm.corpus))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://nlpx.net/archives/57\n",
    "- https://stevenloria.com/tf-idf/\n",
    "- https://ru.wikipedia.org/wiki/TF-IDF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
